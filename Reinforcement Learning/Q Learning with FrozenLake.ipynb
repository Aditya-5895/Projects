{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the hyperparameters ‚öôÔ∏è\n",
    "- Here, we'll specify the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 10000        # Total episodes\n",
    "learning_rate = 0.7           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The Q learning algorithm üß†\n",
    "- Now we implement the Q learning algorithm:\n",
    "<img src=\"qtable_algo.png\" alt=\"Q algo\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4787\n",
      "[[2.72742580e-01 1.27848450e-01 1.22873512e-01 5.16037924e-02]\n",
      " [5.27903276e-02 2.66535868e-03 8.94477376e-05 1.24592669e-01]\n",
      " [1.57056775e-02 1.48950382e-02 2.11678421e-02 1.04147267e-01]\n",
      " [1.43219842e-02 1.33516320e-03 3.19049461e-03 8.57053708e-02]\n",
      " [3.04885157e-01 6.80459921e-02 7.76183517e-02 4.53572917e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.69328258e-01 2.56490430e-05 1.37598051e-03 1.45701969e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.26361020e-02 1.76019221e-01 2.29292409e-02 4.45198318e-01]\n",
      " [6.95565449e-02 6.79868413e-01 1.46928900e-01 7.44936402e-02]\n",
      " [7.27866122e-01 2.98787760e-03 4.82809151e-02 5.22869164e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.01583336e-02 2.42672989e-01 8.78891923e-01 5.90993041e-02]\n",
      " [4.46421602e-01 9.94378294e-01 3.73231518e-01 4.73709175e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        #epsln greedy\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        total_rewards += reward\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 44\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 34\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 14\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 22\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = np.load('frozenlake50k.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.19275933e-02, 1.06638167e-02, 1.29004182e-01, 1.23703189e-02],\n",
       "       [5.66596921e-03, 2.19786584e-02, 5.56448552e-03, 1.52687872e-01],\n",
       "       [7.53786289e-03, 1.06542242e-02, 8.59945709e-03, 1.57668984e-01],\n",
       "       [1.12627468e-02, 2.20352946e-03, 1.54872854e-04, 1.08232483e-01],\n",
       "       [9.74109065e-02, 4.31822782e-02, 4.18546005e-03, 7.49370363e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.39567155e-01, 2.93500747e-06, 5.92865031e-07, 3.34320506e-08],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.42141545e-03, 2.57195005e-02, 5.01695509e-03, 5.25938181e-01],\n",
       "       [8.91795382e-04, 6.62975536e-01, 1.35455681e-03, 7.86434344e-04],\n",
       "       [8.46053756e-01, 1.35333901e-04, 3.62208966e-04, 2.44142604e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.89565276e-02, 2.41107199e-03, 7.47268178e-01, 3.49564210e-02],\n",
       "       [6.14511306e-02, 9.55801090e-01, 2.26958327e-01, 1.15152682e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "1 78\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "1.0\n",
      "Number of steps 78\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "r = 0\n",
    "_ = env.reset()\n",
    "for i in range(2):\n",
    "    r = 0\n",
    "    _ = env.reset()\n",
    "    for step in range(max_steps):        \n",
    "            # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        new_state, reward, done, info = env.step(action)    \n",
    "        r += reward\n",
    "        env.render()\n",
    "        print(i,step)\n",
    "        time.sleep(0.3)\n",
    "        if done:\n",
    "                # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()  \n",
    "            print(r)\n",
    "            time.sleep(2)\n",
    "#             time.sleep(0.1)\n",
    "                # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        clear_output(wait=True)\n",
    "        state = new_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.14991432e-01, 1.27364740e-01, 1.00949066e-01, 1.00788304e-01],\n",
       "       [1.47916905e-02, 1.15377539e-02, 2.06704590e-05, 1.07740871e-01],\n",
       "       [1.04253938e-02, 1.18793387e-02, 9.20682999e-02, 7.20686071e-02],\n",
       "       [1.00683982e-02, 7.79418609e-03, 9.48760992e-04, 6.99295907e-02],\n",
       "       [5.41013124e-01, 3.50847315e-03, 4.74362771e-02, 2.33429349e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [8.22613501e-04, 6.77050935e-05, 2.86096554e-02, 2.70650396e-07],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.43960236e-02, 9.56940555e-03, 1.64824200e-02, 6.28427806e-01],\n",
       "       [6.12204323e-02, 7.86358745e-01, 4.22077988e-02, 2.98263135e-02],\n",
       "       [9.24924486e-01, 5.09685525e-03, 2.17881203e-02, 1.97336607e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.66685499e-02, 1.91222312e-01, 9.22602536e-01, 3.97865245e-03],\n",
       "       [3.96387369e-01, 9.99846295e-01, 2.05963703e-01, 1.76786630e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
